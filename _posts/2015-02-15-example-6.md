---
title: Why Is Overfitting Bad?​
---
    Overfitting is empirically bad. Suppose you have a data set which you split in two, test and training. ... An overfitted model uses more of the noise, which increases its performance in the case of known noise (training data) and decreases its performance in the case of novel noise.​

    Overfitting essentially means taking too much information from your data and using it in a model. To see why this is bad, suppose you split a data set into two sets, test and training. In this case, to say a model is overfitted model means that the model performs significantly worse on the test data set than the training data set.​
